{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro_to_NLP_Fastai_Part-1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7N2YtWeX3cgMs6lcUtukj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashikshafi08/Learning-Fastai/blob/main/Intro_to_NLP/Intro_to_NLP_Fastai_Part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPQK2AOGL08F"
      },
      "source": [
        "# A Code-First Introduction to NLP course by Fastai (Rachel Thomas)\n",
        "\n",
        "Here in this notebook I will jot down the notes and code snippets as I go throught this course. \n",
        "\n",
        "Link for the video tutorials: https://www.youtube.com/playlist?list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9 \n",
        "\n",
        "Github repo for the materials: https://github.com/fastai/course-nlp\n",
        "\n",
        "\n",
        "## Syllabus Topics Covered :\n",
        "\n",
        "### 1. What is NLP?\n",
        "- A changing field\n",
        "- Resources\n",
        "-Tools\n",
        "- Python libraries\n",
        "- Example applications\n",
        "- Ethics issues\n",
        "\n",
        "### 2. Topic Modeling with NMF and SVD\n",
        "\n",
        "- Stop words, stemming, & lemmatization\n",
        "- Term-document matrix\n",
        "- Topic Frequency-Inverse Document Frequency (TF-IDF)\n",
        "- Singular Value Decomposition (SVD)\n",
        "- Non-negative Matrix Factorization (NMF)\n",
        "- Truncated SVD, Randomized SVD\n",
        "### 3. Sentiment classification with Naive Bayes, Logistic regression, and ngrams\n",
        "\n",
        "- Sparse matrix storage\n",
        "- Counters\n",
        "- the fastai library\n",
        "- Naive Bayes\n",
        "- Logistic regression\n",
        "- Ngrams\n",
        "- Logistic regression with Naive Bayes features, with trigrams\n",
        "\n",
        "### 4. Regex (and re-visiting tokenization)\n",
        "\n",
        "### 5. Language modeling & sentiment classification with deep learning\n",
        "\n",
        "- Language model\n",
        "- Transfer learning\n",
        "- Sentiment classification\n",
        "\n",
        "### 6. Translation with RNNs\n",
        "\n",
        "- Review Embeddings\n",
        "- Bleu metric\n",
        "- Teacher Forcing\n",
        "- Bidirectional\n",
        "- Attention\n",
        "\n",
        "### 7. Translation with the Transformer architecture\n",
        "\n",
        "- Transformer Model\n",
        "- Multi-head attention\n",
        "- Masking\n",
        "- Label smoothing\n",
        "\n",
        "### 8. Bias & ethics in NLP\n",
        "- bias in word embeddings\n",
        "- types of bias\n",
        "- attention economy\n",
        "- drowning in fraudulent/fake info\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDENQwQxLrJe"
      },
      "source": [
        "!pip install fastai --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnpD3-LRMRTf",
        "outputId": "1e1251dc-cbe3-4c2a-fbd9-467e1c26672f"
      },
      "source": [
        "import fastai \n",
        "print(fastai.__version__)\n",
        "from fastai import * \n",
        "from fastai.text import *"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ie4jM0lMgDO"
      },
      "source": [
        "## Topic Modelling with NMF and SVD\n",
        "\n",
        "A good way to start the study of NLP, we will use two popular matrix decomposition techniques. \n",
        "\n",
        "We have laid a matrix (**term-document matrix**) of different character names and the acts wrote by Shakespeare, it's an example of bag of words. This is also called Latent Semantic Analysis. \n",
        "\n",
        "term —> names of the characters are considered as terms. \n",
        "\n",
        "Acts —> are considered as documents\n",
        "\n",
        "The dataset we are about to use is consists of, 18000 newsgroups posts with 20 topics. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S4mgqcUST05"
      },
      "source": [
        "# Importing the things we need \n",
        "import numpy as np \n",
        "from sklearn.datasets import fetch_20newsgroups # the dataset we're going to use\n",
        "from sklearn import decomposition\n",
        "from scipy import linalg\n",
        "import matplotlib.pyplot as plt "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVCCuf_BSj4T"
      },
      "source": [
        "np.set_printoptions(suppress=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWZx_F-uSmXs",
        "outputId": "a5bf5183-cc6c-421c-ab59-6936eb08d91e"
      },
      "source": [
        "fetch_20newsgroups"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function sklearn.datasets._twenty_newsgroups.fetch_20newsgroups>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLqlhVJ2TYUj"
      },
      "source": [
        "# Picking only 4 topics \n",
        "categories = ['alt.atheism' , 'talk.religion.misc' , 'comp.graphics' , 'sci.space' ]\n",
        "\n",
        "# Things to be removed \n",
        "remove = ['headers' , 'footers' , 'quotes']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvatTvb8T4m-"
      },
      "source": [
        "# Creating a train and test set \n",
        "newsgroups_train = fetch_20newsgroups(subset= 'train' , \n",
        "                                      categories = categories , \n",
        "                                      remove = remove)\n",
        "newsgroups_test = fetch_20newsgroups(subset = 'test' , \n",
        "                                     categories = categories , \n",
        "                                     remove = remove)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB2jfnDMUNUN",
        "outputId": "933656b5-6d01-4235-9b76-52d045d5f0a2"
      },
      "source": [
        "# Checking the shapes (Post and target)\n",
        "newsgroups_train.filenames.shape , newsgroups_train.target.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2034,), (2034,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_Ipol2kUbDu",
        "outputId": "6286599e-6495-4a5f-d18e-0b36c745b72d"
      },
      "source": [
        "# Checking first 3 example of the filenames (post)\n",
        "print('\\n'.join(newsgroups_train.data[:3]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hi,\n",
            "\n",
            "I've noticed that if you only save a model (with all your mapping planes\n",
            "positioned carefully) to a .3DS file that when you reload it after restarting\n",
            "3DS, they are given a default position and orientation.  But if you save\n",
            "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
            "know why this information is not stored in the .3DS file?  Nothing is\n",
            "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
            "I'd like to be able to read the texture rule information, does anyone have \n",
            "the format for the .PRJ file?\n",
            "\n",
            "Is the .CEL file format available from somewhere?\n",
            "\n",
            "Rych\n",
            "\n",
            "\n",
            "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
            "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
            "folks with him, children and all, to satisfy his delusional mania. Jim\n",
            "Jones, circa 1993.\n",
            "\n",
            "\n",
            "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
            "for centuries.\n",
            "\n",
            " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
            "\n",
            "MB>                                                             So the\n",
            "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
            "\n",
            "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
            "\n",
            "Couldn't we just say periapsis or apoapsis?\n",
            "\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4QPqGbOVAdK",
        "outputId": "0eb663f4-823b-402f-bad8-0c26786c5f6b"
      },
      "source": [
        "# What are targets of the above sentences? \n",
        "np.array(newsgroups_train.target_names)[newsgroups_train.target[:3]]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['comp.graphics', 'talk.religion.misc', 'sci.space'], dtype='<U18')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51y13OSDVs_c",
        "outputId": "8675309e-25b4-4b21-e2c2-11e3652f8ee8"
      },
      "source": [
        "# The target attribute is in index of each category \n",
        "newsgroups_train.target[:10]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 3, 2, 0, 2, 0, 2, 1, 2, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUNTplDkV9jE"
      },
      "source": [
        "# The number of topics we want to look in and top words \n",
        "num_topics , num_top_words = 6 , 8"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocsNyOinWSAL"
      },
      "source": [
        "## Stop words, stemming, lemmatization\n",
        "\n",
        "#### Stopwords \n",
        "https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html\n",
        "\n",
        "Some extremly common words which would appear to be of little value in helping select documents matching a user needs are excluded from the vocabulary entirely. \n",
        "\n",
        "These words are called stopwords. \n",
        "\n",
        "The general trend in IR systems over time has been from standard use of quite large stop lists (200-300 terms) to very small stop lists (7-12 terms) to no stop list whatsoever.\n",
        "\n",
        "**Things I have to read** \n",
        "- https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming\n",
        "- https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87Sq6XveWVYh",
        "outputId": "15a7cf5a-4ce6-45c1-c5a2-5207a83929c6"
      },
      "source": [
        "# Printing out some of the stopwords \n",
        "from sklearn.feature_extraction import stop_words\n",
        "\n",
        "# Displaying 20 stop words\n",
        "sorted(list(stop_words.ENGLISH_STOP_WORDS))[:20]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'across',\n",
              " 'after',\n",
              " 'afterwards',\n",
              " 'again',\n",
              " 'against',\n",
              " 'all',\n",
              " 'almost',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'already',\n",
              " 'also',\n",
              " 'although',\n",
              " 'always',\n",
              " 'am',\n",
              " 'among',\n",
              " 'amongst',\n",
              " 'amoungst']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JumWmGpNXRnM"
      },
      "source": [
        "#### Stemming and Lemmatization \n",
        "https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
        "\n",
        "- Stemming and lemmatization both generates the root form the words. \n",
        "\n",
        "- Lemmatization uses the rules about a language. The resulting tokens are all actual words. \n",
        "\n",
        "- Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.\n",
        "\n",
        "Are the below words the same?\n",
        "\n",
        "*organize, organizes, and organizing*\n",
        "\n",
        "*democracy, democratic, and democratization*\n",
        "\n",
        "> \"Stemming is the poor-man’s lemmatization.\" (Noah Smith, 2011) Stemming is a crude heuristic that chops the ends off of words. The resulting tokens may not be actual words. Stemming is faster.\n",
        "\n",
        "We will use NLTK to demonstrate these types of techniques. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvIdtHfAcOD6"
      },
      "source": [
        "#####NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lI5Qau4BYH2h",
        "outputId": "47660e19-dd96-4406-8a74-ebab758792d5"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkPHBqHOZMpF"
      },
      "source": [
        "from nltk import stem\n",
        "wnl = stem.WordNetLemmatizer() # instantiating lemmatization function\n",
        "porter = stem.porter.PorterStemmer()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY3lMFlvZcxL"
      },
      "source": [
        "# Creating a word list \n",
        "word_list = ['feet' , 'foot' , 'foots' , 'footing']"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgOpL1NHaQXS",
        "outputId": "635e11aa-484f-49a5-c8e6-b69789de35f3"
      },
      "source": [
        "# Performing lemmatization\n",
        "[wnl.lemmatize(word) for word in word_list]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['foot', 'foot', 'foot', 'footing']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hxb3hawzaY52",
        "outputId": "7857c81c-58f1-445f-be17-d7ee6d833ecd"
      },
      "source": [
        "# Performing stemming \n",
        "[porter.stem(word) for word in word_list]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['feet', 'foot', 'foot', 'foot']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znbB5htba3UZ"
      },
      "source": [
        "# Creating lists of words to perform stemming and lemmatization \n",
        "\n",
        "fl_list = ['flies' , 'flying' , 'fly']\n",
        "org_list = ['organize' , 'organizes' , 'organizing']\n",
        "un_list = ['universe' , 'university']"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hab0hlGGbSq5",
        "outputId": "03b7c189-86dc-49a3-9e02-b7b89308723d"
      },
      "source": [
        "# Performing lemmatization on above list \n",
        "print([wnl.lemmatize(word) for word in fl_list])\n",
        "print([wnl.lemmatize(word) for word in org_list])\n",
        "print([wnl.lemmatize(word) for word in un_list])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fly', 'flying', 'fly']\n",
            "['organize', 'organizes', 'organizing']\n",
            "['universe', 'university']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFiUhUTrb3ai",
        "outputId": "fc10aa4e-f607-46ce-d69b-e954f31b0cf0"
      },
      "source": [
        "# Performing stemming on the same list of words \n",
        "print([porter.stem(word) for word in fl_list])\n",
        "print([porter.stem(word) for word in org_list])\n",
        "print([porter.stem(word) for word in un_list])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fli', 'fli', 'fli']\n",
            "['organ', 'organ', 'organ']\n",
            "['univers', 'univers']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeWeZhIAcKAk"
      },
      "source": [
        "##### Spacy\n",
        "\n",
        "Trying out the same with Spacy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adrBH_GOccS_"
      },
      "source": [
        "# Trying out the lemmatization function\n",
        "import spacy\n",
        "from spacy.lemmatizer import Lemmatizer\n",
        "from spacy.lookups import Lookups"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_uxJzwydeyH",
        "outputId": "669218b1-0f70-4e34-b8ae-43f23987fe08"
      },
      "source": [
        "# Creating a instance \n",
        "lookups = Lookups()\n",
        "lemmatizer = Lemmatizer(lookups= lookups)\n",
        "\n",
        "[lemmatizer.lookup(word) for word in word_list]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['feet', 'foot', 'foots', 'footing']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOCdfyqZdst8"
      },
      "source": [
        "Spacy doesn't offer a stemmer (since lemmatization considered better). \n",
        "\n",
        "Also Stopwords vary from library to library "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoHN3c08er-W",
        "outputId": "872f5ef5-ef54-47a6-9412-01dfe8f39d6a"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "sorted(list(nlp.Defaults.stop_words))[:20]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"'d\",\n",
              " \"'ll\",\n",
              " \"'m\",\n",
              " \"'re\",\n",
              " \"'s\",\n",
              " \"'ve\",\n",
              " 'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'across',\n",
              " 'after',\n",
              " 'afterwards',\n",
              " 'again',\n",
              " 'against',\n",
              " 'all',\n",
              " 'almost',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'already',\n",
              " 'also']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GACtyHpPfFW-",
        "outputId": "fd815402-0097-4b8d-ba03-1369300ed355"
      },
      "source": [
        "# Exercise: What stop words appear in spacy but not in sklearn?¶\n",
        "nlp.Defaults.stop_words - stop_words.ENGLISH_STOP_WORDS"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"'d\",\n",
              " \"'ll\",\n",
              " \"'m\",\n",
              " \"'re\",\n",
              " \"'s\",\n",
              " \"'ve\",\n",
              " 'ca',\n",
              " 'did',\n",
              " 'does',\n",
              " 'doing',\n",
              " 'just',\n",
              " 'make',\n",
              " \"n't\",\n",
              " 'n‘t',\n",
              " 'n’t',\n",
              " 'quite',\n",
              " 'really',\n",
              " 'regarding',\n",
              " 'say',\n",
              " 'unless',\n",
              " 'used',\n",
              " 'using',\n",
              " 'various',\n",
              " '‘d',\n",
              " '‘ll',\n",
              " '‘m',\n",
              " '‘re',\n",
              " '‘s',\n",
              " '‘ve',\n",
              " '’d',\n",
              " '’ll',\n",
              " '’m',\n",
              " '’re',\n",
              " '’s',\n",
              " '’ve'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cMhDmZFfb6h",
        "outputId": "69887575-c7a3-477d-9f8f-286a02716259"
      },
      "source": [
        "# Exercise: And what stop words are in sklearn but not spacy?¶\n",
        "stop_words.ENGLISH_STOP_WORDS - nlp.Defaults.stop_words"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "frozenset({'amoungst',\n",
              "           'bill',\n",
              "           'cant',\n",
              "           'co',\n",
              "           'con',\n",
              "           'couldnt',\n",
              "           'cry',\n",
              "           'de',\n",
              "           'describe',\n",
              "           'detail',\n",
              "           'eg',\n",
              "           'etc',\n",
              "           'fill',\n",
              "           'find',\n",
              "           'fire',\n",
              "           'found',\n",
              "           'hasnt',\n",
              "           'ie',\n",
              "           'inc',\n",
              "           'interest',\n",
              "           'ltd',\n",
              "           'mill',\n",
              "           'sincere',\n",
              "           'system',\n",
              "           'thick',\n",
              "           'thin',\n",
              "           'un'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACLPHKjkf5NF"
      },
      "source": [
        "These were long considered standard techniques, but they can often hurt your performance if using deep learning. Stemming, lemmatization, and removing stop words all involve throwing away information.\n",
        "\n",
        "However, they can still be useful when working with simpler models.\n",
        "\n",
        "Sub-word tokens: https://github.com/google/sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXvl6ujmguKi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}