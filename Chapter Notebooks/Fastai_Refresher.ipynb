{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fastai_Refresher.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjppnGCNPtvAZum2F3Yx26",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashikshafi08/Learning-Fastai/blob/main/Chapter%20Notebooks/Fastai_Refresher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uuMuqBjhkvF"
      },
      "source": [
        "This notebooks will cover 4 chapters of the Fastai lectures. After 2 months of break and now I decided I have to go back and finish this book and take proper notes and use my skills in competition. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDh4f6RXrvU3",
        "outputId": "38910b43-c18f-4bfe-ea68-a77614f78202"
      },
      "source": [
        "# Installing fastai \n",
        "!pip install fastai --upgrade "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fastai\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/ca/bc9f4e04adcdfda1357f5c63bc67a7bf4f315883ca544726f3376b1ed068/fastai-2.4-py3-none-any.whl (187kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 18.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 20kB 23.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 30kB 24.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 40kB 18.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 51kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 61kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 71kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 81kB 12.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 92kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 102kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 112kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 122kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 133kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 143kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 153kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 163kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 174kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 184kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 14.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: torch<1.10,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.9.0+cu102)\n",
            "Requirement already satisfied, skipping upgrade: pip in /usr/local/lib/python3.7/dist-packages (from fastai) (19.3.1)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from fastai) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from fastai) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai) (2.2.4)\n",
            "Requirement already satisfied, skipping upgrade: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai) (0.10.0+cu102)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from fastai) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from fastai) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai) (0.22.2.post1)\n",
            "Collecting fastcore<1.4,>=1.3.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b0/f1fbf554e0bf3c76e1bdc3b82eedfe41fcf656479586be38c64421082b1b/fastcore-1.3.20-py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from fastprogress>=0.2.4->fastai) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.10,>=1.7.0->fastai) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (7.4.0)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (57.0.0)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->fastai) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<4->fastai) (4.5.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<4->fastai) (3.4.1)\n",
            "Installing collected packages: fastcore, fastai\n",
            "  Found existing installation: fastai 1.0.61\n",
            "    Uninstalling fastai-1.0.61:\n",
            "      Successfully uninstalled fastai-1.0.61\n",
            "Successfully installed fastai-2.4 fastcore-1.3.20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ4qLqI8rzdk"
      },
      "source": [
        "# Importing the needed libs \n",
        "from fastai import * \n",
        "from fastai.vision.all import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niUdsVcm5U0f"
      },
      "source": [
        "# Under the hood: Training a Digit Training Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "cjSh-6NqtJeK",
        "outputId": "6f291811-ab64-41a5-f454-b3a73b5114ad"
      },
      "source": [
        "# Reading the data\n",
        "path = untar_data(URLs.MNIST_SAMPLE)\n",
        "path.ls()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#3) [Path('/root/.fastai/data/mnist_sample/labels.csv'),Path('/root/.fastai/data/mnist_sample/valid'),Path('/root/.fastai/data/mnist_sample/train')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4VF8J3dr49f",
        "outputId": "fb115dfe-8bbe-4cf0-a2a1-5b8e46c949b0"
      },
      "source": [
        "# Putting into sep variable of 3 and 7 \n",
        "threes = (path / 'train' / '3').ls().sorted()\n",
        "sevens = (path / 'train' / '7').ls().sorted()\n",
        "\n",
        "threes[:5] , sevens[:5]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((#5) [Path('/root/.fastai/data/mnist_sample/train/3/10.png'),Path('/root/.fastai/data/mnist_sample/train/3/10000.png'),Path('/root/.fastai/data/mnist_sample/train/3/10011.png'),Path('/root/.fastai/data/mnist_sample/train/3/10031.png'),Path('/root/.fastai/data/mnist_sample/train/3/10034.png')],\n",
              " (#5) [Path('/root/.fastai/data/mnist_sample/train/7/10002.png'),Path('/root/.fastai/data/mnist_sample/train/7/1001.png'),Path('/root/.fastai/data/mnist_sample/train/7/10014.png'),Path('/root/.fastai/data/mnist_sample/train/7/10019.png'),Path('/root/.fastai/data/mnist_sample/train/7/10039.png')])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ijd622ItD8Z",
        "outputId": "71360672-a12e-4867-fde4-d9401c535802"
      },
      "source": [
        "# Representing in a numpy array \n",
        "img_3 = threes[1]\n",
        "im3 = Image.open(img_3)\n",
        "im3.show()\n",
        "np.array(im3)[4:10 , 4:10] # rows and columns "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0],\n",
              "       [  0,   0,   0,   0,   0,  29],\n",
              "       [  0,   0,   0,  48, 166, 224],\n",
              "       [  0,  93, 244, 249, 253, 187],\n",
              "       [  0, 107, 253, 253, 230,  48],\n",
              "       [  0,   3,  20,  20,  15,   0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oPWetkxuc9p"
      },
      "source": [
        "# Stacking all the images in the directory, and converting into tensors\n",
        "\n",
        "seven_tensor = [tensor(Image.open(o)) for o in sevens]\n",
        "three_tensor = [tensor(Image.open(o)) for o in threes]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BPY3H-ItbP0"
      },
      "source": [
        "Calculate the average over all the images of the intensity of the pixel. \n",
        "\n",
        "Our tensors are in integers let's stack them and convert into float by dividing them by 255. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9LFFRX7t5gc",
        "outputId": "c041ec10-6680-4678-fb61-92cc0132811f"
      },
      "source": [
        "stacked_sevens = torch.stack(seven_tensor).float() / 255\n",
        "stacked_threes = torch.stack(three_tensor).float() / 255\n",
        "\n",
        "stacked_sevens[:1]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0824, 0.2000, 0.8353, 0.9961,\n",
              "          0.9882, 0.9882, 0.9882, 0.9961, 0.9882, 0.9882, 0.9882, 0.9961,\n",
              "          0.9882, 0.9882, 0.9882, 1.0000, 0.9882, 0.3922, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0824, 0.6314, 0.9804, 0.9804, 0.9882,\n",
              "          0.9804, 0.9804, 0.9804, 0.9882, 0.9804, 0.9804, 0.9804, 0.9882,\n",
              "          0.9804, 0.9804, 0.9804, 0.9882, 0.9804, 0.3922, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.2000, 0.9804, 0.9804, 0.9804, 0.9882,\n",
              "          0.7412, 0.7451, 0.9804, 0.9882, 0.9804, 0.9804, 0.9804, 0.9882,\n",
              "          0.9804, 0.9804, 0.9804, 0.9882, 0.7412, 0.1569, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0392, 0.5098, 0.9804, 0.9804, 0.1922,\n",
              "          0.1137, 0.1176, 0.1922, 0.1922, 0.1922, 0.1922, 0.1922, 0.1922,\n",
              "          0.1922, 0.6667, 0.9804, 0.9882, 0.5843, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0431,\n",
              "          0.5176, 0.9882, 0.9882, 0.9569, 0.4745, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000,\n",
              "          0.9804, 0.9804, 0.9804, 0.7922, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6745,\n",
              "          0.9804, 0.9804, 0.9804, 0.3137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9882,\n",
              "          0.9804, 0.9804, 0.9804, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1216, 0.8353, 0.9961,\n",
              "          0.9882, 0.9882, 0.1922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5922, 0.9804, 0.9882,\n",
              "          0.9804, 0.9804, 0.1922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5922, 0.9804, 0.9882,\n",
              "          0.9804, 0.6235, 0.0784, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5922, 0.9804, 0.9882,\n",
              "          0.9804, 0.3922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5961, 0.9882, 0.9961,\n",
              "          0.9882, 0.3922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5922, 0.9804, 0.9882,\n",
              "          0.9804, 0.3922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5922, 0.9804, 0.9882,\n",
              "          0.9804, 0.3922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5922, 0.9804, 0.9882,\n",
              "          0.9804, 0.8667, 0.1569, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6000, 0.9882, 1.0000,\n",
              "          0.9882, 0.9882, 0.1922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5922, 0.9804, 0.9882,\n",
              "          0.9804, 0.9804, 0.1922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2353, 0.8667, 0.9882,\n",
              "          0.9804, 0.6235, 0.0784, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569, 0.8314,\n",
              "          0.1922, 0.0784, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZnOMywsvL1g",
        "outputId": "0cdcdaac-4c87-45ed-c0f0-65b30ccf463a"
      },
      "source": [
        "# Checking the shape \n",
        "stacked_sevens.shape , stacked_threes.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([6265, 28, 28]), torch.Size([6131, 28, 28]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "7M-FvMESvVb4",
        "outputId": "005193c1-b082-45a8-96c0-7b838e063b04"
      },
      "source": [
        "# Finding the ideal 3 by taking average along the 0th dimension\n",
        "mean_3 = stacked_threes.mean(dim = 0)\n",
        "mean_7 = stacked_sevens.mean(dim = 0)\n",
        "show_image(mean_3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe249556350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 72x72 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXmRIBwfvmfb"
      },
      "source": [
        "Now we have two stuffs, one the ideal image and the actual image. We have to calculate the distance between them. \n",
        "\n",
        "- Mean absolute difference --> replaces negative with positive values\n",
        "- Mean Squared Error --> makes everything positive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLAYiuCywT7L",
        "outputId": "99520e36-e20a-4ebd-afc7-1e7e934b6f28"
      },
      "source": [
        "# Creating a vlid 3 and 7 tensors\n",
        "\n",
        "valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path / 'valid' / '3').ls()])\n",
        "valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path / 'valid' / '7').ls()])\n",
        "\n",
        "# Converting them into 0 and 1 \n",
        "valid_3_tens = valid_3_tens.float() / 255\n",
        "valid_7_tens = valid_7_tens.float() / 255 \n",
        "\n",
        "# Shape of the valid tensors \n",
        "valid_3_tens.shape , valid_7_tens.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9VYtdVrxhC0",
        "outputId": "e4e2309b-d65a-47d1-8261-e5d8d20d2a22"
      },
      "source": [
        "# Writing the function which cal dist btw ideal and arbitrary value\n",
        "# taking the mean ranging over the values indexed by the last two axes of the tensors\n",
        "def mnist_distance(a , b):\n",
        "  return (a - b).abs().mean((-1 , -2))\n",
        "\n",
        "# Using the function \n",
        "valid_3_dist = mnist_distance(valid_3_tens , mean_3)\n",
        "valid_3_dist.shape \n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1010])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYOIHoMIy2Pi",
        "outputId": "a4a57b61-ea2d-411e-be6e-fb49a7f8f8c4"
      },
      "source": [
        "# The first 5 sample's distance \n",
        "valid_3_dist[:5]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1321, 0.1371, 0.1018, 0.1375, 0.1044])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmJUQYF8y7Tz",
        "outputId": "538496f6-0b8f-45ae-c00c-88b9bb72e581"
      },
      "source": [
        "# Taking one sample from 3 and 7 \n",
        "a_3 = stacked_threes[5]\n",
        "a_7 = stacked_sevens[6]\n",
        "\n",
        "len(a_3) , len(a_7)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC1h4dwzzUdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "330c929d-bf4a-4a18-a5ec-3e81912a94b0"
      },
      "source": [
        "# Calculating the distance between ideal and single image file \n",
        "mnist_distance(a_3 , mean_3) , mnist_distance(a_7 , mean_7)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1700), tensor(0.1542))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ghwS9bT044X",
        "outputId": "3cb647e2-c2d0-40ec-8a9a-08bc41bf4967"
      },
      "source": [
        "# Function to check whether a tensor is 3 or not \n",
        "def is_3(x):\n",
        "  '''\n",
        "  the distance should between the tensor and the mean 3 should be less than the mean 7, we can call it as a 3\n",
        "  '''\n",
        "  return mnist_distance(x , mean_3) < mnist_distance(x , mean_7)\n",
        "\n",
        "# Passing one tensor \n",
        "is_3(a_3) # this is 3 and should return 3 "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5RffPDF3xSM",
        "outputId": "f2178f8e-4d9a-437c-cff7-6dff9d810c8c"
      },
      "source": [
        "# Passing our whole valid 3 set and see how it goes \n",
        "is_3(valid_3_tens)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ True,  True,  True,  ...,  True, False, False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbserpWV4MlV",
        "outputId": "2f7de0d8-b686-457f-8e46-96e28c0c60bf"
      },
      "source": [
        "# Alright now calculate the distance for 3 and 7 \n",
        "accuracy_3s = is_3(valid_3_tens).float().mean()\n",
        "\n",
        "# For 7 we will take the inverse of all the 7s (since we don't have is_7 function)\n",
        "accuracy_7s = (1 - (is_3(valid_7_tens).float().mean()))\n",
        "\n",
        "accuracy_3s , accuracy_7s"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.9168), tensor(0.9854))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k__n3fR949zF",
        "outputId": "93248919-1bf9-44fd-c65c-0b0e6dcd0acb"
      },
      "source": [
        "# It will perform worse since it's 7 \n",
        "is_3(valid_7_tens).float().mean()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0146)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WpDeltn5GiM"
      },
      "source": [
        "## SGD \n",
        "\n",
        "- This is something that will allow our model to get better and better which gives the **ability of learning to the model**.\n",
        "\n",
        "**How do we make it work?**\n",
        "- Assign a weight \n",
        "- Tweak the weight (parameters) and improve based on the weight assignment.\n",
        "\n",
        "**What can we do to our pixel similarity in order to apply SGD (or a optimizer)?**\n",
        "- Assign a weight for each pixel values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azmZCGxP6Rv-"
      },
      "source": [
        "# Probability of a number being 8\n",
        "def pr_eight(x , w):\n",
        "  '''\n",
        "  w = weights for the pixels \n",
        "  x = input image \n",
        "  '''\n",
        "  return (x*w).sum()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ecuPnNH6fIz"
      },
      "source": [
        "The above function will help us to update the weight `w` for every iteration and make our weight assignemnt better and better. \n",
        "\n",
        "> Converting the above function `pr_8` to a machine learning classifier: \n",
        "- Initialize the weights \n",
        "- For each image, use the weights that was initialized and predict whether it appears to be a 3 or a 7. (Step 2)\n",
        "- Based on the predictions, **calculate how good the model is** (*Calculte loss*). \n",
        "- Tweak (step) the weights based on the above calculation. \n",
        "- Go back again to Step 2, where you use the weights and make predictions. \n",
        "- Iterate this process until we decide to stop the training. \n",
        "\n",
        "#### Guidelines\n",
        "\n",
        "- **Initialize**\n",
        "\n",
        "    We initialize the parameters (or) weights to random values at first. It's believed starting with random weights (or) values works perfectly well. \n",
        "\n",
        "- **Loss**\n",
        "\n",
        "    A function will return a number that is small when the performance of the model is good. The standard approach is to treat a **small loss as a good and large loss as bad.** \n",
        "\n",
        "- **Step**\n",
        "\n",
        "    A simple way to figure out whether a weight should be increased a bit or decreased, would be just try to increase the **weight** by a small amount and observe the loss goes up or down. We do this **increment and decrement until we find an amount that satisfy us**. \n",
        "\n",
        "    However, we use calculus to take care of this. Finding which direction and roughly how much, to change each weight without doing those adjustments above. \n",
        "\n",
        "    We do this by calculating ***gradients.*** This is just an **performance optimization.**\n",
        "\n",
        "- **Stop**\n",
        "\n",
        "    This is the phase where we choose the epochs to train the model for, we would keep training until the accuracy of the model started getting worse or ran out of time.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY227g6t7jBg"
      },
      "source": [
        "#### Calculating Gradients \n",
        "\n",
        "In here we will use calculus as a performance optimizartion. \n",
        "\n",
        "**But why ?**\n",
        "- It will help us quickly to calculate whether our loss will go up and down as we are adjusting the parameters.\n",
        "\n",
        "> Gradients will tell us how much we have to change each weight to make our model better\n",
        "\n",
        "**What the hell is a derivative?**\n",
        "- it calculates the change of a equation rather a value.\n",
        "- For instance, the derivative of the quadratic function at the value 3 tells us how rapidly the function changes at the value 3. \n",
        "\n",
        "**Exact definition of a gradient**\n",
        "\n",
        "*Gradient is defined as rise/run that is the change in the value of the function, divided by the change in the value of the parameter.*\n",
        "\n",
        "The takeaway: \n",
        "- When we know how our function would change, then we know what we need to do in order to make it smaller (loss function). \n",
        "- The key is having a function and change the parameter of the function to make the loss smaller. \n",
        "\n",
        "#### Things to know\n",
        "\n",
        "- The function will return not one but alot of weights, so when we calculate the derivative we will get alots of number, `i.e gradient for every weight`. \n",
        "- `requires_grad_()` special method tells pytorch we want to calculate gradients w.r.t to the variable at the value. For instance, in 3x (x). By doing this Pytorch, will keep track of all the computed gradients. \n",
        "- `backward` --> backpropagation, this is process of calculating the derivative (gradients) for each layer. \n",
        "- In `backward pass` we calculate the gradients of a neural network, and on `forward pass` we calculate the activations of a neural net.\n",
        "\n",
        "> **Backpropagation** is a training algorithm consisting of 2 steps: \n",
        "1. Feed forward the values \n",
        "2. Calculate the error and propagate it back to the earlier layers. So to be precise, forward-propagation is part of the backpropagation algorithm but comes before back-propagating.\n",
        "\n",
        "\n",
        "- https://datascience.stackexchange.com/questions/66416/forward-pass-vs-backward-pass-vs-backpropagation\n",
        "- https://stackoverflow.com/questions/28403782/what-is-the-difference-between-back-propagation-and-feed-forward-neural-network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UWWcBEu9_Yl",
        "outputId": "a6fe4bbb-f046-40df-9d64-f65148fc267c"
      },
      "source": [
        "# Calculating Gradients with Pytorch\n",
        "\n",
        "import torch \n",
        "\n",
        "# Creating a tensor (will keep track of the gradients of the value\n",
        "xt = tensor(8.).requires_grad_()\n",
        "\n",
        "# Sample function \n",
        "def f(x): return x**2\n",
        "\n",
        "# Performing some computations with xt\n",
        "yt = f(xt)\n",
        "yt"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(64., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w1-PKKNCjLS"
      },
      "source": [
        "# Telling pytoch to calculate the gradients by calling grad \n",
        "yt.backward() # always pass a function"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Jnk7_VvFmLn",
        "outputId": "c6685ca5-d1a6-4f00-ce94-e29aed7f217e"
      },
      "source": [
        "# Now viewing the gradients calculated on our variable \n",
        "xt.grad # now get the gradient on a variable"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(16.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB2KexQ8Fxk0"
      },
      "source": [
        "The graidents will tell us only the slope of our function, they don't really say how far we should adjust the parameters. \n",
        "\n",
        "https://en.wikipedia.org/wiki/Slope#Calculus\n",
        "\n",
        "- If slope is very large --> More adjustments to do \n",
        "- If slope is very small --> We are close to the optimal value. \n",
        "\n",
        "### Stepping (way to increase/decrease weight) with a Learning Rate \n",
        "- This is the idea of **multiplying the gradient by a small number** (that is the learning rate).\n",
        "- We can adjust the learning rate by, `w- = w.grad * lr`. \n",
        "\n",
        "This means we get the gradients and multiply the gradients with a learning rate. \n",
        "\n",
        "- If the learning rate is too low, optimization will take a lot of time because steps towards the minimum of the loss function are tiny.\n",
        "- If the learning rate is too high, it can result in getting the *loss* worse. Rather than diverging (or) converging it will bounce around. \n",
        "https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny3uebqcGfIa"
      },
      "source": [
        "### An End-to-End SGD Example \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRfSrslPMiMI",
        "outputId": "eb83e105-d032-4d95-cbe2-f3d82fac246f"
      },
      "source": [
        "# Time \n",
        "time = torch.arange(0 , 20).float()\n",
        "# Time in seconds \n",
        "time"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
              "        14., 15., 16., 17., 18., 19.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "d0t5Egj3Mnqc",
        "outputId": "fdb06d56-0f12-4b43-d424-4c56d64c6fe9"
      },
      "source": [
        "# Calculating the speed -> a*(t**2) + (b*t) + c\n",
        "speed = torch.randn(20)*3 + 0.75*(time -9.5)**2 + 1\n",
        "\n",
        "# Plotting the time and speed\n",
        "plt.scatter(time , speed)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fe1c309efd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFnGkvwxM-yO"
      },
      "source": [
        "def f(t , params):\n",
        "  a , b , c = params\n",
        "  return a*(t**2) + (b*t) + c"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pH2O7tCOL52"
      },
      "source": [
        "Every quadratic function returns 3 values a, b and c. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS-6susNOiLK"
      },
      "source": [
        "# Since we're on a continous data our loss function would be mse\n",
        "def mse(preds , targs):\n",
        "  return ((preds - targs)**2).mean()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scsKo6IaO4Ow",
        "outputId": "6b327d22-d5da-4f3c-e9bd-b9ac1810abde"
      },
      "source": [
        "# Firstly --> Initialize the parameters\n",
        "params = torch.randn(3).requires_grad_()\n",
        "params # a b c"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7312, 0.0652, 0.3453], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgJvercAPPFZ",
        "outputId": "2ca22522-3210-43bb-aea1-37ebd6b12ee9"
      },
      "source": [
        "# Calculating the predictions \n",
        "preds = f(time , params)\n",
        "preds"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  0.3453,   1.1417,   3.4006,   7.1220,  12.3059,  18.9522,  27.0610,\n",
              "         36.6323,  47.6660,  60.1622,  74.1209,  89.5420, 106.4256, 124.7717,\n",
              "        144.5802, 165.8513, 188.5847, 212.7807, 238.4391, 265.5600],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "nii6rWSaTbsJ",
        "outputId": "b4c05cd9-b450-4b3a-b7ed-0205988713d1"
      },
      "source": [
        "# A function to plot both targets and preds to see how close our predictins are\n",
        "\n",
        "def show_preds(preds , ax=None):\n",
        "  if ax is None: ax=plt.subplots()[1]\n",
        "  ax.scatter(time , speed)\n",
        "  ax.scatter(time , to_np(preds) , color='red')\n",
        "  ax.set_ylim(-300 , 100)\n",
        "\n",
        "show_preds(preds)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUmm-D7_TfWq",
        "outputId": "719e9a7f-31b7-4c83-8f61-66d7779bd517"
      },
      "source": [
        "# Now the next step --> Calculate the loss\n",
        "loss = mse(preds , speed)\n",
        "loss"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10705.3359, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYkSZQmrTz3B",
        "outputId": "9819ffd6-275f-4660-9179-05644e907a82"
      },
      "source": [
        "# Calculating the gradients (from the loss function)\n",
        "loss.backward()\n",
        "params.grad # for a,b,c we get the gradients"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([33782.0156,  2168.4062,   131.2081])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A8NsKmRUbcr",
        "outputId": "51a5af9e-b393-4aca-f5fb-cba5302a884e"
      },
      "source": [
        "# Just multiplying lr with gradients \n",
        "params.grad * 1e-5"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.3378, 0.0217, 0.0013])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5e624jJT-ot"
      },
      "source": [
        "# Stepping --> update the gradients with a learnin rate \n",
        "lr = 1e-5 # learning rate \n",
        "\n",
        "# Functionality that's help us in updating the learning rate \n",
        "params.data -= lr * params.grad.data\n",
        "params.grad = None\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4eKsGJXVVpR"
      },
      "source": [
        "# Putting everything into a function \n",
        "def apply_step(params , prn = True):\n",
        "  # Getting the predictions \n",
        "  preds = f(time , params)\n",
        "  # Calculate the loss \n",
        "  loss = mse(preds , speed)\n",
        "  # Initiating back prop on loss function \n",
        "  loss.backward()\n",
        "  # Stepping with a learning rate (multiplying the gradients with a lr)\n",
        "  params.data -= lr * params.grad.data\n",
        "  params.grad = None\n",
        "  if prn:\n",
        "    # Getting only the numbers (we don't want the tensors)\n",
        "    print(loss.item())\n",
        "  return preds\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU2rhhj2XtSb",
        "outputId": "c7f78318-4f6b-4197-c160-4737cba2dff0"
      },
      "source": [
        "# Iterating \n",
        "for i in range(10):\n",
        "  apply_step(params)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2483.153076171875\n",
            "927.2630615234375\n",
            "632.8387451171875\n",
            "577.1221923828125\n",
            "566.5765380859375\n",
            "564.5784912109375\n",
            "564.1980590820312\n",
            "564.1237182617188\n",
            "564.1072387695312\n",
            "564.1016235351562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKJI0rQfX1Di"
      },
      "source": [
        "#### Summarizing Gradient Descent\n",
        "\n",
        "- At beginning, the weights of our model can be random (or) from pre-trained model.\n",
        "- We compare the model with our targets and prediction using a **loss function,** which returns a number that we want to make as low as possible by **improving our weights.**\n",
        "- To find how to change the **weights** to make the loss a bit better, we use calculus to **calculate the gradients.**\n",
        "- Calculating gradients is similar finding a steepest downward slope, we use the **magnitude of the gradient** (steepness of a slope) to tell us how big a step to take.\n",
        "- To decide on the step size, we multiply the gradient by a number we choose called the **learning rate.**\n",
        "- We then iterate until we have reached the lower point, and then stop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5eKqeHqYM0h",
        "outputId": "8b9f2243-1d8b-4106-8cbf-4bce90c46274"
      },
      "source": [
        "# Changing them from a list of matrices (rank 3) to a list of vectors (rank 2)\n",
        "\n",
        "train_x = torch.cat([stacked_threes , stacked_sevens]).view(-1 , 28*28)\n",
        "train_x.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12396, 784])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfU3C9X1ZQjB"
      },
      "source": [
        ">`-1` —> denotes the row, since we don't know how many rows exactly in a dataset (or) this image, we use -1. Which says make this axis as big as necessary to fit all the data. Like we do in slicing!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFYUecR4ZW6G",
        "outputId": "edac99c8-161c-4230-f819-e4c412a658ca"
      },
      "source": [
        "# Constructing our labels (3s -> 1 & 7s -> 0s)\n",
        "\n",
        "train_y = tensor([1] * len(threes) + [0] * len(sevens)).unsqueeze(1)\n",
        "train_y.shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12396, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HttCCq-PZhYt",
        "outputId": "4425db93-5c9b-4da5-ff5b-d235bf1d7ed8"
      },
      "source": [
        "# Zipping x and y into a dataset \n",
        "dset = list(zip(train_x , train_y))\n",
        "x , y = dset[0] # take one sample\n",
        "\n",
        "# Checking the shape \n",
        "x.shape , y.shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([784]), torch.Size([1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ0urfk9Zv4l",
        "outputId": "193aeca4-5376-498e-d5a7-3d4b440712f8"
      },
      "source": [
        "# x and y \n",
        "\n",
        "print(f'Images in Tensor (sliced) : {x[0 : 100]}')\n",
        "print('----------------- ------------ --------- -------')\n",
        "print(f'Labels: {y}')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Images in Tensor (sliced) : tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.])\n",
            "----------------- ------------ --------- -------\n",
            "Labels: tensor([1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYPS8e_OZ2z1"
      },
      "source": [
        "# Doing the exact thing for our validation set \n",
        "\n",
        "valid_x = torch.cat([valid_3_tens , valid_7_tens]).view(-1 , 28*28)\n",
        "valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\n",
        "\n",
        "# Putting them into a dataset \n",
        "valid_dset = list(zip(valid_x , valid_y))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh_ztb-haIHa"
      },
      "source": [
        "**Initialize the parameters with random numbers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMWsqr5FaMHT"
      },
      "source": [
        "# Generate weight for every pixel \n",
        "def init_params(size , std = 1.0):\n",
        "  return (torch.randn(size) * std).requires_grad_()\n",
        "\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr7y2mXtatRr",
        "outputId": "b54429f9-2636-42cb-fb98-60bfa3387137"
      },
      "source": [
        "# Using the function and creating random weights \n",
        "weights = init_params(size = (28*28 , 1) , std=1.0)\n",
        "weights.shape"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([784, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqNV1yb3a_DY"
      },
      "source": [
        "# Initializing bias \n",
        "\n",
        "bias = init_params(1)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGjAbkVFbDBc",
        "outputId": "ba707eb5-60b7-4237-e3f5-e20a6439f528"
      },
      "source": [
        "# Prediction on one image \n",
        "(train_x[0] * weights.T).sum() + bias\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-10.5789], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRRVFphabkpU"
      },
      "source": [
        "Alright so far we have made a initializer function that gives us random weights, we got: \n",
        "- Function to calculate the predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2LU4YMkBjr-",
        "outputId": "a3cc0616-4646-499f-dd6f-5d079fcbf07c"
      },
      "source": [
        "# Constructing the matrix multiplication function (y = wx + b)\n",
        "\n",
        "# As we can say our tiny model indeed \n",
        "def linear1(xb):\n",
        "  '''\n",
        "  xb --> Input training batch\n",
        "\n",
        "  We return by multiplying the weights to our each input mini-batch\n",
        "  '''\n",
        "  return xb@weights + bias\n",
        "\n",
        "# Getting the predictions for all images in the training set (x)\n",
        "preds = linear1(train_x)\n",
        "preds[:10]"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-10.5789],\n",
              "        [ -5.4708],\n",
              "        [-12.0206],\n",
              "        [ -9.9986],\n",
              "        [ -3.7133],\n",
              "        [-10.1751],\n",
              "        [ -7.9517],\n",
              "        [-12.3929],\n",
              "        [  0.3624],\n",
              "        [  0.3678]], grad_fn=<SliceBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mWmLm7rCYOm"
      },
      "source": [
        "Looking back before jumping into calculus and other stuffs first we make a prediction on our input data, then observe how the model/function performs? Not well? \n",
        "\n",
        "Alright we will decide that by passing the preds and targs into a loss function. And we know our goal is to minimize the loss so now we we will get the help of calculus (gradients) that will help our loss function to find the minimum. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVjfs3TcDEUg"
      },
      "source": [
        "# Calculating the accuracy\n",
        "corrects = (preds > 0.0).float() == train_y"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjC4c1YBDR2O"
      },
      "source": [
        "# Manually tweaking one value of the weight matrix \n",
        "with torch.no_grad():\n",
        "  weights[0] *= 1.001\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So3Pu87LK_sp"
      },
      "source": [
        "Making a change with just `weights[0] *=1.001` throws an error, this is because we are trying to access even the graidents and make a change in that. \n",
        "\n",
        "By using with `torch.no_grad()` we can turn of the gradients and make computations just on the value of the matrices. \n",
        "\n",
        "https://datascience.stackexchange.com/questions/32651/what-is-the-use-of-torch-no-grad-in-pytorch\n",
        "\n",
        "Gradients tells the loss function how to make changes in order to tweak the weights. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6y7oqR6DSiD"
      },
      "source": [
        " \n",
        "\n",
        "Reads: \n",
        "- https://discuss.pytorch.org/t/what-is-the-purpose-of-is-leaf/87000/5\n",
        "- https://stackoverflow.com/questions/57188409/assigning-a-parameter-to-the-gpu-sets-is-leaf-as-false\n",
        "- https://forums.fast.ai/t/weights-0-in-place-operations/89308\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-QbwOf0EZo0",
        "outputId": "8ad78066-05cc-4c65-e8a6-906ccf1893c0"
      },
      "source": [
        "# Making the prediction after manually tweaking the value \n",
        "preds = linear1(train_x)\n",
        "\n",
        "((preds > 0.0).float() == train_y).float().mean().item()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40496933460235596"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02x4ogu2MYAJ"
      },
      "source": [
        "If we are going to use accuracy as an performance metric the slight changes in the weight won't make a difference in the loss function. \n",
        "\n",
        "So we need a metrics where little changes on the weight matrice should make a impact on the loss function. \n",
        "\n",
        "> We need a loss function that when our weights result in slightly better predictions, gives us a slightly better loss.\n",
        "\n",
        "The only thing our model will do is, given a number predicts it whether an 3 or not. if it's not 3 then it has to be 7. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vq14I66IdpU"
      },
      "source": [
        "# Calculating the loss \n",
        "trgts = tensor([1 , 0 , 1])\n",
        "prds = tensor([0.9 , 0.4 , 0.2])\n",
        "\n",
        "# Creating our own loss function \n",
        "def mnist_loss(predictions , targets):\n",
        "  return torch.where(targets == 1 , 1 - predictions , predictions).mean()"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRvyTYJ2M_2a",
        "outputId": "1dfcb213-9e40-49e0-d338-66b2bb65586e"
      },
      "source": [
        "mnist_loss(prds , trgts)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.4333)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTGqhz2PO9Ni"
      },
      "source": [
        "# Importing utils from the fastbook \n",
        "\n",
        "#!pip install -q fastbook \n",
        "#from fastbook import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "-9BKCgYZNeLe",
        "outputId": "fabe9171-535e-4d47-e247-fa7051110ed4"
      },
      "source": [
        "# Example of sigmoid from Pytorch function\n",
        "\n",
        "plot_function(torch.sigmoid , title='Sigmoid' , min=-4 , max=4)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastbook/__init__.py:73: UserWarning: Not providing a value for linspace's steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at  /pytorch/aten/src/ATen/native/RangeFactories.cpp:25.)\n",
            "  x = torch.linspace(min,max)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CEUFMyRN0EQ"
      },
      "source": [
        "Sigmoid function squeezes the values between the range 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoG0tkS_Omkl"
      },
      "source": [
        "# Updating our above mnist_loss with a sigmoid function \n",
        "def mnist_loss(predictions, targets):\n",
        "  return torch.where(targets == 1 , 1-predictions ,predictions).mean()"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJkuVbkXPnw8"
      },
      "source": [
        "metric --> For human understanding \n",
        "loss --> For automatic updation (machine's metric to improve the performance of our model)\n",
        "\n",
        "The loss function is calculated for each item in our dataset, and then at the end of an epoch, the loss values are all averaged and the overall mean is reported for the epoch.\n",
        "\n",
        "Metrics on other hand are the numbers we care about and these are the values which are printed at end of each epoch that tells us how our model is doing.\n",
        "\n",
        "\n",
        "### SGD and Mini-Batches \n",
        "\n",
        "Now in here we will try to automate most of the parts. \n",
        "\n",
        "- Batching a handful items into a seperate mini-batches and making computations on those mini-batches rather than computing on the whole dataset / items.\n",
        "-Choosing a good batch size is one of the decisions one need to make a deep learning practitioner to train the model quickly and accurately.\n",
        "- The important reason for having mini-batches is it could run on GPU, so the computations takes place even more faster. \n",
        "\n",
        "\n",
        "Before putting our data into batches, \n",
        "- We gotta randomly shuffle the data. \n",
        "\n",
        "\n",
        "- it prevents any bias during the training\n",
        "- it prevents the model from learning the order of the training\n",
        "- it helps the training converge fast\n",
        "\n",
        "https://stats.stackexchange.com/questions/245502/why-should-we-shuffle-data-while-training-a-neural-network\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-fC_h4zQB9K"
      },
      "source": [
        "#### DataLoader\n",
        "\n",
        "A `DataLoader` can take any Python collection and turn it into a iterator (generator) over many batches. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWuKgzl7WUNx",
        "outputId": "309c95c7-bc46-41aa-839f-40c882a00b5e"
      },
      "source": [
        "# Sample DataLoader \n",
        "\n",
        "coll = range(20) \n",
        "dls = DataLoader(coll , batch_size = 5 , shuffle = True)\n",
        "list(dls)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([ 7, 17,  5,  9, 15]),\n",
              " tensor([19, 11, 10, 13,  2]),\n",
              " tensor([12,  0, 18, 16,  3]),\n",
              " tensor([ 1,  4,  6, 14,  8])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXmsaDI_WvG1"
      },
      "source": [
        "`Dataset` --> Contains collection of both independent and dependent variable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKtlvhsTXArd",
        "outputId": "d319e98d-916c-41bc-dbf6-cc11fc4aebab"
      },
      "source": [
        "# Example of Dataset \n",
        "ds = L(enumerate(string.ascii_lowercase))\n",
        "ds"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7, 'h'),(8, 'i'),(9, 'j')...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXxFDBstXKpK"
      },
      "source": [
        "In practice, we pass a `Dataset` to a `DataLoader` and it returns batches of independent and dependent variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "081C7xmRXZFh"
      },
      "source": [
        "### Putting it all together \n",
        "\n",
        "- Initialize the parameters \n",
        "- Making a prediction \n",
        "- Depending upon my prediction, will calculate my loss function.\n",
        "- Call our good friend gradients, activate the gradients of our weights.\n",
        "- We are going to perform to step, where we will multiply our learning with our weight gradients. \n",
        "- We will train this for our desired epochs. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlhLm6EOY8g8"
      },
      "source": [
        "# re-initializing the parameters \n",
        "weights = init_params((28*28 , 1))\n",
        "bias = init_params(1)\n"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmmdglZWcqAe",
        "outputId": "8a820da5-c528-4b21-9833-84d7f4a96d2e"
      },
      "source": [
        "# Constructing our DataLoader \n",
        "dl = DataLoader(dset , batch_size= 256)\n",
        "# Slicing off first mini-batch\n",
        "xb , yb = first(dl)\n",
        "\n",
        "# Checking the shape \n",
        "xb.shape , yb.shape"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([256, 784]), torch.Size([256, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GebvSZW6dTO_"
      },
      "source": [
        "# Doing the same for validaton set \n",
        "valid_dl = DataLoader(valid_dset , batch_size = 256)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOsVsZsMdg5H",
        "outputId": "c8ca3acd-b3db-4c17-b7b0-e355af22e728"
      },
      "source": [
        "# Create the mini-batch of size 4 \n",
        "batch = train_x[:4]\n",
        "batch.shape"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 784])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCK__wf0dsE-",
        "outputId": "41fc792d-1d81-4262-e3a4-1ee82650533e"
      },
      "source": [
        "# Making predictions on our sample batch \n",
        "preds = linear1(batch)\n",
        "preds\n"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 4.7141],\n",
              "        [13.3941],\n",
              "        [ 7.3772],\n",
              "        [10.1254]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GunWe6ce_0A",
        "outputId": "b60f4ff0-5b66-4ad0-dec8-4e1624189465"
      },
      "source": [
        "train_y[:4]"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDK5ma6EdtQ-",
        "outputId": "4a5d7d5b-55d9-4d23-a480-52a73bfa720f"
      },
      "source": [
        "# Calculating the loss \n",
        "loss = mnist_loss(preds , train_y[:4])\n",
        "loss "
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-7.9027, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Avt9NCs8fHRJ",
        "outputId": "28361fec-9ded-4493-bb81-cfbe628d3ac4"
      },
      "source": [
        "# Now calculating the gradients \n",
        "loss.backward()\n",
        "print(f'Shape of weight calculated: {weights.grad.shape}')\n",
        "print(f'Turning into a single tensor: {weights.grad.mean()}')\n",
        "print(f'Bias: {bias.grad}')"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of weight calculated: torch.Size([784, 1])\n",
            "Turning into a single tensor: -0.15112045407295227\n",
            "Bias: tensor([-1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8786Y4q9ftRv"
      },
      "source": [
        "# Putting altogether in a function \n",
        "def calc_grad(xb , yb , model):\n",
        "  '''\n",
        "  xb --> mini-batched training set \n",
        "  yb --> mini-batched test set \n",
        "  model --> the model we want to use \n",
        "  '''\n",
        "  preds = model(xb)\n",
        "  loss = mnist_loss(preds , yb)\n",
        "  loss.backward() \n",
        "  \n"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRHr0Eykhd2G",
        "outputId": "5724a4f2-e338-4911-83d9-983e19d1222e"
      },
      "source": [
        "# Testing the above function \n",
        "calc_grad(batch , train_y[:4] , linear1)\n",
        "\n",
        "# Below are the weights and biases after calculating the gradients above \n",
        "weights.grad.mean() , bias.grad"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-0.3022), tensor([-2.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdrwVulNhz1g",
        "outputId": "a81a0d5f-49f5-4cf3-c387-d9f17a683aa4"
      },
      "source": [
        "# Calling it another time\n",
        "calc_grad(batch , train_y[:4] , linear1)\n",
        "\n",
        "# Below are wts and biases\n",
        "weights.grad.mean() , bias.grad"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-0.4534), tensor([-3.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9gtJ5AeiCgB",
        "outputId": "6eb08f70-1c08-42fc-d530-7a98911e2a43"
      },
      "source": [
        "# Making the gradients to zero, so it won't affect the previous computer gradients \n",
        "weights.grad.zero_()\n",
        "bias.grad.zero_()"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c3E362GifK7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}