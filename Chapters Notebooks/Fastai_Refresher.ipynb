{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fastai_Refresher.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMODhUVU8qt6diu2MQUzgBt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashikshafi08/Learning-Fastai/blob/main/Chapters%20Notebooks/Fastai_Refresher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uuMuqBjhkvF"
      },
      "source": [
        "This notebooks will cover 4 chapters of the Fastai lectures. After 2 months of break and now I decided I have to go back and finish this book and take proper notes and use my skills in competition. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDh4f6RXrvU3",
        "outputId": "743dff89-c9d9-448d-935b-caa587791cf3"
      },
      "source": [
        "# Installing fastai \n",
        "!pip install fastai --upgrade "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fastai\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/ca/bc9f4e04adcdfda1357f5c63bc67a7bf4f315883ca544726f3376b1ed068/fastai-2.4-py3-none-any.whl (187kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 14.4MB/s eta 0:00:01\r\u001b[K     |███▌                            | 20kB 20.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 30kB 17.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 40kB 15.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 51kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 61kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 71kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 81kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 92kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 102kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 112kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 122kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 133kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 143kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 153kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 163kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 174kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 184kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pip in /usr/local/lib/python3.7/dist-packages (from fastai) (19.3.1)\n",
            "Requirement already satisfied, skipping upgrade: torch<1.10,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.9.0+cu102)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from fastai) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai) (0.10.0+cu102)\n",
            "Requirement already satisfied, skipping upgrade: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai) (2.2.4)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from fastai) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai) (3.2.2)\n",
            "Collecting fastcore<1.4,>=1.3.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b0/f1fbf554e0bf3c76e1bdc3b82eedfe41fcf656479586be38c64421082b1b/fastcore-1.3.20-py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from fastai) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from fastai) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from fastprogress>=0.2.4->fastai) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.10,>=1.7.0->fastai) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (7.4.0)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (57.0.0)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<4->fastai) (4.5.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->fastai) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<4->fastai) (3.4.1)\n",
            "Installing collected packages: fastcore, fastai\n",
            "  Found existing installation: fastai 1.0.61\n",
            "    Uninstalling fastai-1.0.61:\n",
            "      Successfully uninstalled fastai-1.0.61\n",
            "Successfully installed fastai-2.4 fastcore-1.3.20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ4qLqI8rzdk"
      },
      "source": [
        "# Importing the needed libs \n",
        "from fastai import * \n",
        "from fastai.vision.all import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niUdsVcm5U0f"
      },
      "source": [
        "# Under the hood: Training a Digit Training Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "cjSh-6NqtJeK",
        "outputId": "003d25b1-c2a2-423d-930e-76dc5c732820"
      },
      "source": [
        "# Reading the data\n",
        "path = untar_data(URLs.MNIST_SAMPLE)\n",
        "path.ls()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#3) [Path('/root/.fastai/data/mnist_sample/train'),Path('/root/.fastai/data/mnist_sample/valid'),Path('/root/.fastai/data/mnist_sample/labels.csv')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4VF8J3dr49f",
        "outputId": "4163772b-4e47-4efe-afbe-23bd1896726a"
      },
      "source": [
        "# Putting into sep variable of 3 and 7 \n",
        "threes = (path / 'train' / '3').ls().sorted()\n",
        "sevens = (path / 'train' / '7').ls().sorted()\n",
        "\n",
        "threes[:5] , sevens[:5]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((#5) [Path('/root/.fastai/data/mnist_sample/train/3/10.png'),Path('/root/.fastai/data/mnist_sample/train/3/10000.png'),Path('/root/.fastai/data/mnist_sample/train/3/10011.png'),Path('/root/.fastai/data/mnist_sample/train/3/10031.png'),Path('/root/.fastai/data/mnist_sample/train/3/10034.png')],\n",
              " (#5) [Path('/root/.fastai/data/mnist_sample/train/7/10002.png'),Path('/root/.fastai/data/mnist_sample/train/7/1001.png'),Path('/root/.fastai/data/mnist_sample/train/7/10014.png'),Path('/root/.fastai/data/mnist_sample/train/7/10019.png'),Path('/root/.fastai/data/mnist_sample/train/7/10039.png')])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ijd622ItD8Z",
        "outputId": "e93ed450-5f13-4870-df5b-767ab76f51ef"
      },
      "source": [
        "# Representing in a numpy array \n",
        "img_3 = threes[1]\n",
        "im3 = Image.open(img_3)\n",
        "im3.show()\n",
        "np.array(im3)[4:10 , 4:10] # rows and columns "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0],\n",
              "       [  0,   0,   0,   0,   0,  29],\n",
              "       [  0,   0,   0,  48, 166, 224],\n",
              "       [  0,  93, 244, 249, 253, 187],\n",
              "       [  0, 107, 253, 253, 230,  48],\n",
              "       [  0,   3,  20,  20,  15,   0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oPWetkxuc9p"
      },
      "source": [
        "# Stacking all the images in the directory, and converting into tensors\n",
        "\n",
        "seven_tensor = [tensor(Image.open(o)) for o in sevens]\n",
        "three_tensor = [tensor(Image.open(o)) for o in threes]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BPY3H-ItbP0"
      },
      "source": [
        "Calculate the average over all the images of the intensity of the pixel. \n",
        "\n",
        "Our tensors are in integers let's stack them and convert into float by dividing them by 255. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9LFFRX7t5gc",
        "outputId": "81415813-8ea5-4f19-ffbb-dcc281df13a3"
      },
      "source": [
        "stacked_sevens = torch.stack(seven_tensor).float() / 255\n",
        "stacked_threes = torch.stack(three_tensor).float() / 255\n",
        "\n",
        "stacked_sevens[:1]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0824, 0.2000, 0.8353, 0.9961,\n",
              "          0.9882, 0.9882, 0.9882, 0.9961, 0.9882, 0.9882, 0.9882, 0.9961,\n",
              "          0.9882, 0.9882, 0.9882, 1.0000, 0.9882, 0.3922, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0824, 0.6314, 0.9804, 0.9804, 0.9882,\n",
              "          0.9804, 0.9804, 0.9804, 0.9882, 0.9804, 0.9804, 0.9804, 0.9882,\n",
              "          0.9804, 0.9804, 0.9804, 0.9882, 0.9804, 0.3922, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.2000, 0.9804, 0.9804, 0.9804, 0.9882,\n",
              "          0.7412, 0.7451, 0.9804, 0.9882, 0.9804, 0.9804, 0.9804, 0.9882,\n",
              "          0.9804, 0.9804, 0.9804, 0.9882, 0.7412, 0.1569, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0392, 0.5098, 0.9804, 0.9804, 0.1922,\n",
              "          0.1137, 0.1176, 0.1922, 0.1922, 0.1922, 0.1922, 0.1922, 0.1922,\n",
              "          0.1922, 0.6667, 0.9804, 0.9882, 0.5843, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0431,\n",
              "          0.5176, 0.9882, 0.9882, 0.9569, 0.4745, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000,\n",
              "          0.9804, 0.9804, 0.9804, 0.7922, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6745,\n",
              "          0.9804, 0.9804, 0.9804, 0.3137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9882,\n",
              "          0.9804, 0.9804, 0.9804, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1216, 0.8353, 0.9961,\n",
              "          0.9882, 0.9882, 0.1922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5922, 0.9804, 0.9882,\n",
              "          0.9804, 0.9804, 0.1922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5922, 0.9804, 0.9882,\n",
              "          0.9804, 0.6235, 0.0784, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5922, 0.9804, 0.9882,\n",
              "          0.9804, 0.3922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5961, 0.9882, 0.9961,\n",
              "          0.9882, 0.3922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5922, 0.9804, 0.9882,\n",
              "          0.9804, 0.3922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5922, 0.9804, 0.9882,\n",
              "          0.9804, 0.3922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5922, 0.9804, 0.9882,\n",
              "          0.9804, 0.8667, 0.1569, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6000, 0.9882, 1.0000,\n",
              "          0.9882, 0.9882, 0.1922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5922, 0.9804, 0.9882,\n",
              "          0.9804, 0.9804, 0.1922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2353, 0.8667, 0.9882,\n",
              "          0.9804, 0.6235, 0.0784, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569, 0.8314,\n",
              "          0.1922, 0.0784, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZnOMywsvL1g",
        "outputId": "be5afbaa-b426-4bf1-9c88-953be6211bea"
      },
      "source": [
        "# Checking the shape \n",
        "stacked_sevens.shape , stacked_threes.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([6265, 28, 28]), torch.Size([6131, 28, 28]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "7M-FvMESvVb4",
        "outputId": "a0ecc4a6-61a8-40fa-ec78-a95dbfb1311a"
      },
      "source": [
        "# Finding the ideal 3 by taking average along the 0th dimension\n",
        "mean_3 = stacked_threes.mean(dim = 0)\n",
        "mean_7 = stacked_sevens.mean(dim = 0)\n",
        "show_image(mean_3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd082eb6810>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 72x72 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXmRIBwfvmfb"
      },
      "source": [
        "Now we have two stuffs, one the ideal image and the actual image. We have to calculate the distance between them. \n",
        "\n",
        "- Mean absolute difference --> replaces negative with positive values\n",
        "- Mean Squared Error --> makes everything positive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLAYiuCywT7L",
        "outputId": "c113437b-c05c-4585-d5d4-859251423b83"
      },
      "source": [
        "# Creating a vlid 3 and 7 tensors\n",
        "\n",
        "valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path / 'valid' / '3').ls()])\n",
        "valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path / 'valid' / '7').ls()])\n",
        "\n",
        "# Converting them into 0 and 1 \n",
        "valid_3_tens = valid_3_tens.float() / 255\n",
        "valid_7_tens = valid_7_tens.float() / 255 \n",
        "\n",
        "# Shape of the valid tensors \n",
        "valid_3_tens.shape , valid_7_tens.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9VYtdVrxhC0",
        "outputId": "c19c822b-1d2e-448a-d0e1-1c5b4c4cad20"
      },
      "source": [
        "# Writing the function which cal dist btw ideal and arbitrary value\n",
        "# taking the mean ranging over the values indexed by the last two axes of the tensors\n",
        "def mnist_distance(a , b):\n",
        "  return (a - b).abs().mean((-1 , -2))\n",
        "\n",
        "# Using the function \n",
        "valid_3_dist = mnist_distance(valid_3_tens , mean_3)\n",
        "valid_3_dist.shape \n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1010])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYOIHoMIy2Pi",
        "outputId": "0880a437-ded6-442d-e0c8-f57236048eed"
      },
      "source": [
        "# The first 5 sample's distance \n",
        "valid_3_dist[:5]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1286, 0.1097, 0.1295, 0.1211, 0.1142])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmJUQYF8y7Tz",
        "outputId": "c4d2c116-910e-4ba8-e43a-31641d7abd52"
      },
      "source": [
        "# Taking one sample from 3 and 7 \n",
        "a_3 = stacked_threes[5]\n",
        "a_7 = stacked_sevens[6]\n",
        "\n",
        "len(a_3) , len(a_7)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC1h4dwzzUdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be5c5509-c0b8-48ef-f3e9-5640ce008399"
      },
      "source": [
        "# Calculating the distance between ideal and single image file \n",
        "mnist_distance(a_3 , mean_3) , mnist_distance(a_7 , mean_7)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1700), tensor(0.1542))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ghwS9bT044X",
        "outputId": "ff841b9c-1e84-4440-b37c-c90c8f1697dd"
      },
      "source": [
        "# Function to check whether a tensor is 3 or not \n",
        "def is_3(x):\n",
        "  '''\n",
        "  the distance should between the tensor and the mean 3 should be less than the mean 7, we can call it as a 3\n",
        "  '''\n",
        "  return mnist_distance(x , mean_3) < mnist_distance(x , mean_7)\n",
        "\n",
        "# Passing one tensor \n",
        "is_3(a_3) # this is 3 and should return 3 "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5RffPDF3xSM",
        "outputId": "e16ad14a-3f5e-4ddf-d4cf-2a0408738768"
      },
      "source": [
        "# Passing our whole valid 3 set and see how it goes \n",
        "is_3(valid_3_tens)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True,  ..., True, True, True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbserpWV4MlV",
        "outputId": "022e6746-ac88-4256-8ed0-71b11cb2dcd7"
      },
      "source": [
        "# Alright now calculate the distance for 3 and 7 \n",
        "accuracy_3s = is_3(valid_3_tens).float().mean()\n",
        "\n",
        "# For 7 we will take the inverse of all the 7s (since we don't have is_7 function)\n",
        "accuracy_7s = (1 - (is_3(valid_7_tens).float().mean()))\n",
        "\n",
        "accuracy_3s , accuracy_7s"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.9168), tensor(0.9854))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k__n3fR949zF",
        "outputId": "daf6de17-d849-4a1d-f48b-4b1bb7aa5689"
      },
      "source": [
        "# It will perform worse since it's 7 \n",
        "is_3(valid_7_tens).float().mean()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0146)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WpDeltn5GiM"
      },
      "source": [
        "## SGD \n",
        "\n",
        "- This is something that will allow our model to get better and better which gives the **ability of learning to the model**.\n",
        "\n",
        "**How do we make it work?**\n",
        "- Assign a weight \n",
        "- Tweak the weight (parameters) and improve based on the weight assignment.\n",
        "\n",
        "**What can we do to our pixel similarity in order to apply SGD (or a optimizer)?**\n",
        "- Assign a weight for each pixel values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azmZCGxP6Rv-"
      },
      "source": [
        "# Probability of a number being 8\n",
        "def pr_eight(x , w):\n",
        "  '''\n",
        "  w = weights for the pixels \n",
        "  x = input image \n",
        "  '''\n",
        "  return (x*w).sum()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ecuPnNH6fIz"
      },
      "source": [
        "The above function will help us to update the weight `w` for every iteration and make our weight assignemnt better and better. \n",
        "\n",
        "> Converting the above function `pr_8` to a machine learning classifier: \n",
        "- Initialize the weights \n",
        "- For each image, use the weights that was initialized and predict whether it appears to be a 3 or a 7. (Step 2)\n",
        "- Based on the predictions, **calculate how good the model is** (*Calculte loss*). \n",
        "- Tweak (step) the weights based on the above calculation. \n",
        "- Go back again to Step 2, where you use the weights and make predictions. \n",
        "- Iterate this process until we decide to stop the training. \n",
        "\n",
        "#### Guidelines\n",
        "\n",
        "- **Initialize**\n",
        "\n",
        "    We initialize the parameters (or) weights to random values at first. It's believed starting with random weights (or) values works perfectly well. \n",
        "\n",
        "- **Loss**\n",
        "\n",
        "    A function will return a number that is small when the performance of the model is good. The standard approach is to treat a **small loss as a good and large loss as bad.** \n",
        "\n",
        "- **Step**\n",
        "\n",
        "    A simple way to figure out whether a weight should be increased a bit or decreased, would be just try to increase the **weight** by a small amount and observe the loss goes up or down. We do this **increment and decrement until we find an amount that satisfy us**. \n",
        "\n",
        "    However, we use calculus to take care of this. Finding which direction and roughly how much, to change each weight without doing those adjustments above. \n",
        "\n",
        "    We do this by calculating ***gradients.*** This is just an **performance optimization.**\n",
        "\n",
        "- **Stop**\n",
        "\n",
        "    This is the phase where we choose the epochs to train the model for, we would keep training until the accuracy of the model started getting worse or ran out of time.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY227g6t7jBg"
      },
      "source": [
        "#### Calculating Gradients \n",
        "\n",
        "In here we will use calculus as a performance optimizartion. \n",
        "\n",
        "**But why ?**\n",
        "- It will help us quickly to calculate whether our loss will go up and down as we are adjusting the parameters.\n",
        "\n",
        "> Gradients will tell us how much we have to change each weight to make our model better\n",
        "\n",
        "**What the hell is a derivative?**\n",
        "- it calculates the change of a equation rather a value.\n",
        "- For instance, the derivative of the quadratic function at the value 3 tells us how rapidly the function changes at the value 3. \n",
        "\n",
        "**Exact definition of a gradient**\n",
        "\n",
        "*Gradient is defined as rise/run that is the change in the value of the function, divided by the change in the value of the parameter.*\n",
        "\n",
        "The takeaway: \n",
        "- When we know how our function would change, then we know what we need to do in order to make it smaller (loss function). \n",
        "- The key is having a function and change the parameter of the function to make the loss smaller. \n",
        "\n",
        "#### Things to know\n",
        "\n",
        "- The function will return not one but alot of weights, so when we calculate the derivative we will get alots of number, `i.e gradient for every weight`. \n",
        "- `requires_grad_()` special method tells pytorch we want to calculate gradients w.r.t to the variable at the value. For instance, in 3x (x). By doing this Pytorch, will keep track of all the computed gradients. \n",
        "- `backward` --> backpropagation, this is process of calculating the derivative (gradients) for each layer. \n",
        "- In `backward pass` we calculate the gradients of a neural network, and on `forward pass` we calculate the activations of a neural net.\n",
        "\n",
        "> **Backpropagation** is a training algorithm consisting of 2 steps: \n",
        "1. Feed forward the values \n",
        "2. Calculate the error and propagate it back to the earlier layers. So to be precise, forward-propagation is part of the backpropagation algorithm but comes before back-propagating.\n",
        "\n",
        "\n",
        "- https://datascience.stackexchange.com/questions/66416/forward-pass-vs-backward-pass-vs-backpropagation\n",
        "- https://stackoverflow.com/questions/28403782/what-is-the-difference-between-back-propagation-and-feed-forward-neural-network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UWWcBEu9_Yl",
        "outputId": "abcf749d-aa40-4460-e2f3-59c0de44699f"
      },
      "source": [
        "# Calculating Gradients with Pytorch\n",
        "\n",
        "import torch \n",
        "\n",
        "# Creating a tensor (will keep track of the gradients of the value\n",
        "xt = tensor(8.).requires_grad_()\n",
        "\n",
        "# Sample function \n",
        "def f(x): return x**2\n",
        "\n",
        "# Performing some computations with xt\n",
        "yt = f(xt)\n",
        "yt"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(64., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w1-PKKNCjLS"
      },
      "source": [
        "# Telling pytoch to calculate the gradients by calling grad \n",
        "yt.backward() # always pass a function"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Jnk7_VvFmLn",
        "outputId": "09b2cdb2-d0ca-4970-c88a-3801c1ccd0c0"
      },
      "source": [
        "# Now viewing the gradients calculated on our variable \n",
        "xt.grad # now get the gradient on a variable"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(16.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB2KexQ8Fxk0"
      },
      "source": [
        "The graidents will tell us only the slope of our function, they don't really say how far we should adjust the parameters. \n",
        "\n",
        "https://en.wikipedia.org/wiki/Slope#Calculus\n",
        "\n",
        "- If slope is very large --> More adjustments to do \n",
        "- If slope is very small --> We are close to the optimal value. \n",
        "\n",
        "### Stepping (way to increase/decrease weight) with a Learning Rate \n",
        "- This is the idea of **multiplying the gradient by a small number** (that is the learning rate).\n",
        "- We can adjust the learning rate by, `w- = w.grad * lr`. \n",
        "\n",
        "This means we get the gradients and multiply the gradients with a learning rate. \n",
        "\n",
        "- If the learning rate is too low, optimization will take a lot of time because steps towards the minimum of the loss function are tiny.\n",
        "- If the learning rate is too high, it can result in getting the *loss* worse. Rather than diverging (or) converging it will bounce around. \n",
        "https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny3uebqcGfIa"
      },
      "source": [
        "### An End-to-End SGD Example \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRfSrslPMiMI",
        "outputId": "5020a1d4-686b-4908-c4c6-c71fa12d3026"
      },
      "source": [
        "# Time \n",
        "time = torch.arange(0 , 20).float()\n",
        "# Time in seconds \n",
        "time"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
              "        14., 15., 16., 17., 18., 19.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "d0t5Egj3Mnqc",
        "outputId": "cd20f029-4499-44e9-9186-60840481513c"
      },
      "source": [
        "# Calculating the speed -> a*(t**2) + (b*t) + c\n",
        "speed = torch.randn(20)*3 + 0.75*(time -9.5)**2 + 1\n",
        "\n",
        "# Plotting the time and speed\n",
        "plt.scatter(time , speed)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fd0816be310>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFnGkvwxM-yO"
      },
      "source": [
        "def f(t , params):\n",
        "  a , b , c = params\n",
        "  return a*(t**2) + (b*t) + c"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pH2O7tCOL52"
      },
      "source": [
        "Every quadratic function returns 3 values a, b and c. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS-6susNOiLK"
      },
      "source": [
        "# Since we're on a continous data our loss function would be mse\n",
        "def mse(preds , targs):\n",
        "  return ((preds - targs)**2).mean()"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scsKo6IaO4Ow",
        "outputId": "91c2a6d9-7072-4b3a-a000-6f39197174fa"
      },
      "source": [
        "# Firstly --> Initialize the parameters\n",
        "params = torch.randn(3).requires_grad_()\n",
        "params # a b c"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.7114,  0.7888,  1.1415], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgJvercAPPFZ",
        "outputId": "6a3366ba-21ed-4619-cfba-3c11fcf70254"
      },
      "source": [
        "# Calculating the predictions \n",
        "preds = f(time , params)\n",
        "preds"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.1415e+00,  1.2188e+00, -1.2667e-01, -2.8950e+00, -7.0861e+00,\n",
              "        -1.2700e+01, -1.9737e+01, -2.8197e+01, -3.8079e+01, -4.9384e+01,\n",
              "        -6.2112e+01, -7.6263e+01, -9.1837e+01, -1.0883e+02, -1.2725e+02,\n",
              "        -1.4710e+02, -1.6836e+02, -1.9105e+02, -2.1516e+02, -2.4069e+02],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "nii6rWSaTbsJ",
        "outputId": "cefeb236-9273-4ffb-cb86-5199b928f632"
      },
      "source": [
        "# A function to plot both targets and preds to see how close our predictins are\n",
        "\n",
        "def show_preds(preds , ax=None):\n",
        "  if ax is None: ax=plt.subplots()[1]\n",
        "  ax.scatter(time , speed)\n",
        "  ax.scatter(time , to_np(preds) , color='red')\n",
        "  ax.set_ylim(-300 , 100)\n",
        "\n",
        "show_preds(preds)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUmm-D7_TfWq",
        "outputId": "fa174137-6996-489e-ec2d-02f2d443e7d9"
      },
      "source": [
        "# Now the next step --> Calculate the loss\n",
        "loss = mse(preds , speed)\n",
        "loss"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(18265.4141, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYkSZQmrTz3B",
        "outputId": "2a0d4ad9-8b32-400a-f257-b7f73a0a3a0e"
      },
      "source": [
        "# Calculating the gradients (from the loss function)\n",
        "loss.backward()\n",
        "params.grad # for a,b,c we get the gradients"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-44494.1328,  -2841.1079,   -211.4301])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A8NsKmRUbcr",
        "outputId": "f371e06a-a4ba-40d9-a172-e1ef4be7f02e"
      },
      "source": [
        "# Just multiplying lr with gradients \n",
        "params.grad * 1e-5"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.4449, -0.0284, -0.0021])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5e624jJT-ot"
      },
      "source": [
        "# Stepping --> update the gradients with a learnin rate \n",
        "lr = 1e-5 # learning rate \n",
        "\n",
        "# Functionality that's help us in updating the learning rate \n",
        "params.data -= lr * params.grad.data\n",
        "params.grad = None\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVXC7zsSWp3-"
      },
      "source": [
        "torch.randn()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4eKsGJXVVpR"
      },
      "source": [
        "# Putting everything into a function \n",
        "def apply_step(params , prn = True):\n",
        "  # Getting the predictions \n",
        "  preds = f(time , params)\n",
        "  # Calculate the loss \n",
        "  loss = mse(preds , speed)\n",
        "  # Initiating back prop on loss function \n",
        "  loss.backward()\n",
        "  # Stepping with a learning rate (multiplying the gradients with a lr)\n",
        "  params.data -= lr * params.grad.data\n",
        "  params.grad = None\n",
        "  if prn:\n",
        "    # Getting only the numbers (we don't want the tensors)\n",
        "    print(loss.item())\n",
        "  return preds\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU2rhhj2XtSb",
        "outputId": "b47b70c1-cb64-4ea9-dd46-babab3e78465"
      },
      "source": [
        "# Iterating \n",
        "for i in range(10):\n",
        "  apply_step(params)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "970.9100341796875\n",
            "729.910400390625\n",
            "684.3020629882812\n",
            "675.66796875\n",
            "674.0302734375\n",
            "673.716796875\n",
            "673.6536254882812\n",
            "673.6380004882812\n",
            "673.6312255859375\n",
            "673.6262817382812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKJI0rQfX1Di"
      },
      "source": [
        "#### Summarizing Gradient Descent\n",
        "\n",
        "- At beginning, the weights of our model can be random (or) from pre-trained model.\n",
        "- We compare the model with our targets and prediction using a **loss function,** which returns a number that we want to make as low as possible by **improving our weights.**\n",
        "- To find how to change the **weights** to make the loss a bit better, we use calculus to **calculate the gradients.**\n",
        "- Calculating gradients is similar finding a steepest downward slope, we use the **magnitude of the gradient** (steepness of a slope) to tell us how big a step to take.\n",
        "- To decide on the step size, we multiply the gradient by a number we choose called the **learning rate.**\n",
        "- We then iterate until we have reached the lower point, and then stop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5eKqeHqYM0h",
        "outputId": "3b33e253-1173-4f8b-ff0e-1baef20f2784"
      },
      "source": [
        "# Changing them from a list of matrices (rank 3) to a list of vectors (rank 2)\n",
        "\n",
        "train_x = torch.cat([stacked_threes , stacked_sevens]).view(-1 , 28*28)\n",
        "train_x.shape"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12396, 784])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfU3C9X1ZQjB"
      },
      "source": [
        ">`-1` —> denotes the row, since we don't know how many rows exactly in a dataset (or) this image, we use -1. Which says make this axis as big as necessary to fit all the data. Like we do in slicing!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFYUecR4ZW6G",
        "outputId": "ca1443e9-12c8-49f8-f335-b59e2d9c1da3"
      },
      "source": [
        "# Constructing our labels (3s -> 1 & 7s -> 0s)\n",
        "\n",
        "train_y = tensor([1] * len(threes) + [0] * len(sevens)).unsqueeze(1)\n",
        "train_y.shape"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12396, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HttCCq-PZhYt",
        "outputId": "b2c42955-c7dc-4b5c-f13f-1207f128e488"
      },
      "source": [
        "# Zipping x and y into a dataset \n",
        "dset = list(zip(train_x , train_y))\n",
        "x , y = dset[0] # take one sample\n",
        "\n",
        "# Checking the shape \n",
        "x.shape , y.shape"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([784]), torch.Size([1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ0urfk9Zv4l",
        "outputId": "ded21901-675f-4a47-fc2c-97b710e4099e"
      },
      "source": [
        "# x and y \n",
        "\n",
        "print(f'Images in Tensor (sliced) : {x[0 : 100]}')\n",
        "print('----------------- ------------ --------- -------')\n",
        "print(f'Labels: {y}')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Images in Tensor (sliced) : tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.])\n",
            "----------------- ------------ --------- -------\n",
            "Labels: tensor([1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYPS8e_OZ2z1"
      },
      "source": [
        "# Doing the exact thing for our validation set \n",
        "\n",
        "valid_x = torch.cat([valid_3_tens , valid_7_tens]).view(-1 , 28*28)\n",
        "valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\n",
        "\n",
        "# Putting them into a dataset \n",
        "valid_dset = list(zip(valid_x , valid_y))"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh_ztb-haIHa"
      },
      "source": [
        "**Initialize the parameters with random numbers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMWsqr5FaMHT"
      },
      "source": [
        "# Generate weight for every pixel \n",
        "def init_params(size , std = 1.0):\n",
        "  return (torch.randn(size) * std).requires_grad_()\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr7y2mXtatRr",
        "outputId": "56e41b50-fc20-49a6-af6a-24b82ce26ddc"
      },
      "source": [
        "# Using the function and creating random weights \n",
        "weights = init_params(size = (28*28 , 1) , std=1.0)\n",
        "weights[:10]"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.1387],\n",
              "        [ 0.1346],\n",
              "        [ 0.0981],\n",
              "        [-0.9315],\n",
              "        [ 1.0499],\n",
              "        [-0.1715],\n",
              "        [-0.9364],\n",
              "        [-0.5061],\n",
              "        [-1.1606],\n",
              "        [ 0.4634]], grad_fn=<SliceBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqNV1yb3a_DY"
      },
      "source": [
        "# Initializing bias \n",
        "\n",
        "bias = init_params(1)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGjAbkVFbDBc",
        "outputId": "3532bce0-91e0-4408-998f-1fb45d19e510"
      },
      "source": [
        "# Prediction on one image \n",
        "(train_x[5] * weights.T).sum() + bias\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.3300], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lL3AO8JbQuv"
      },
      "source": [
        "weights.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRRVFphabkpU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}